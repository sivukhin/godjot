package djot_tokenizer

import (
	"github.com/sivukhin/godjot/tokenizer"
)

var (
	DollarByteMask                 = tokenizer.NewByteMask([]byte("$"))
	BacktickByteMask               = tokenizer.NewByteMask([]byte("`"))
	SmartSymbolByteMask            = tokenizer.NewByteMask([]byte("\n'\""))
	AlphaNumericSymbolByteMask     = tokenizer.NewByteMask([]byte("+-0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"))
	AsciiPunctuationSymbolByteMask = tokenizer.NewByteMask([]byte("!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"))
)

func MatchInlineToken(
	r tokenizer.TextReader,
	s tokenizer.ReaderState,
	tokenType DjotToken,
) (tokenizer.ReaderState, bool) {
	fail := func() (tokenizer.ReaderState, bool) { return 0, false }

	switch tokenType {
	case ImageSpanInline:
		return r.Token2(s, [...]byte{'!', '['})
	case SpanInline:
		return r.Token1(s, [...]byte{'['})
	case SpanInline ^ tokenizer.Open, ImageSpanInline ^ tokenizer.Open:
		return r.Token1(s, [...]byte{']'})
	case LinkUrlInline:
		return r.Token1(s, [...]byte{'('})
	case LinkUrlInline ^ tokenizer.Open:
		return r.Token1(s, [...]byte{')'})
	case LinkReferenceInline:
		return r.Token1(s, [...]byte{'['})
	case LinkReferenceInline ^ tokenizer.Open:
		return r.Token1(s, [...]byte{']'})
	case AutolinkInline:
		return r.Token1(s, [...]byte{'<'})
	case AutolinkInline ^ tokenizer.Open:
		return r.Token1(s, [...]byte{'>'})
	case VerbatimInline:
		next, ok := r.MaskRepeat(s, DollarByteMask, 0)
		tokenizer.Assertf(ok, "MaskRepeat must match because minCount is zero")
		// $ - inline math, $$ - display math, more dollars - unknown syntax
		if next-s > 2 {
			return fail()
		}
		return r.MaskRepeat(next, BacktickByteMask, 1)
	case VerbatimInline ^ tokenizer.Open:
		return r.MaskRepeat(s, BacktickByteMask, 1)
	case EmphasisInline:
		if next, ok := r.Token2(s, [...]byte{'{', '_'}); ok {
			return next, ok
		}
		if next, ok := r.Token1(s, [...]byte{'_'}); ok && !r.HasMask(next, tokenizer.SpaceNewLineByteMask) {
			return next, ok
		}
		return fail()
	case EmphasisInline ^ tokenizer.Open:
		if next, ok := r.Token2(s, [...]byte{'_', '}'}); ok {
			return next, ok
		}
		if next, ok := r.Token1(s, [...]byte{'_'}); ok && !r.HasMask(s-1, tokenizer.SpaceNewLineByteMask) {
			return next, ok
		}
		return fail()
	case StrongInline:
		if next, ok := r.Token2(s, [...]byte{'{', '*'}); ok {
			return next, ok
		}
		if next, ok := r.Token1(s, [...]byte{'*'}); ok && !r.HasMask(next, tokenizer.SpaceNewLineByteMask) {
			return next, ok
		}
		return fail()
	case StrongInline ^ tokenizer.Open:
		if next, ok := r.Token2(s, [...]byte{'*', '}'}); ok {
			return next, ok
		}
		if next, ok := r.Token1(s, [...]byte{'*'}); ok && !r.HasMask(s-1, tokenizer.SpaceNewLineByteMask) {
			return next, ok
		}
		return fail()
	case HighlightedInline:
		return r.Token2(s, [...]byte{'{', '='})
	case HighlightedInline ^ tokenizer.Open:
		return r.Token2(s, [...]byte{'=', '}'})
	case SuperscriptInline:
		if next, ok := r.Token2(s, [...]byte{'{', '^'}); ok {
			return next, ok
		}
		return r.Token1(s, [...]byte{'^'})
	case SuperscriptInline ^ tokenizer.Open:
		if next, ok := r.Token2(s, [...]byte{'^', '}'}); ok {
			return next, ok
		}
		return r.Token1(s, [...]byte{'^'})
	case SubscriptInline:
		if next, ok := r.Token2(s, [...]byte{'{', '~'}); ok {
			return next, ok
		}
		return r.Token1(s, [...]byte{'~'})
	case SubscriptInline ^ tokenizer.Open:
		if next, ok := r.Token2(s, [...]byte{'~', '}'}); ok {
			return next, ok
		}
		return r.Token1(s, [...]byte{'~'})
	case InsertInline:
		return r.Token2(s, [...]byte{'{', '+'})
	case InsertInline ^ tokenizer.Open:
		return r.Token2(s, [...]byte{'+', '}'})
	case DeleteInline:
		return r.Token2(s, [...]byte{'{', '-'})
	case DeleteInline ^ tokenizer.Open:
		return r.Token2(s, [...]byte{'-', '}'})
	case FootnoteReferenceInline:
		return r.Token2(s, [...]byte{'[', '^'})
	case FootnoteReferenceInline ^ tokenizer.Open:
		return r.Token1(s, [...]byte{']'})
	case EscapedSymbolInline:
		next, ok := r.Token1(s, [...]byte{'\\'})
		if !ok {
			return fail()
		}
		if r.IsEmpty(next) {
			return fail()
		}
		if asciiNext, ok := r.Mask(next, AsciiPunctuationSymbolByteMask); ok {
			return asciiNext, ok
		}
		next, ok = r.MaskRepeat(next, tokenizer.SpaceByteMask, 0)
		tokenizer.Assertf(ok, "MaskRepeat must match because minCount is zero")
		return r.Token1(next, [...]byte{'\n'})
	case RawFormatInline:
		return r.Token2(s, [...]byte{'{', '='})
	case RawFormatInline ^ tokenizer.Open:
		return r.Token1(s, [...]byte{'}'})
	case SymbolsInline:
		next, ok := r.Token1(s, [...]byte{':'})
		if !ok {
			return fail()
		}
		if word, ok := r.MaskRepeat(next, AlphaNumericSymbolByteMask, 0); ok && r.HasToken(word, ":") {
			return next, true
		}
		return fail()
	case SymbolsInline ^ tokenizer.Open:
		return r.Token1(s, [...]byte{':'})
	case SmartSymbolInline:
		if next, ok := r.Token1(s, [...]byte{'{'}); ok {
			return r.Mask(next, SmartSymbolByteMask)
		}
		if next, ok := r.Mask(s, SmartSymbolByteMask); ok {
			if r.HasToken(next, "}") {
				return next + 1, true
			}
			return next, true
		}
		if next, ok := r.Token(s, "..."); ok {
			return next, ok
		}
		if next, ok := r.ByteRepeat(s, '-', 2); ok {
			return next, ok
		}
		return fail()
	case PipeTableSeparator:
		next, ok := r.Token1(s, [...]byte{'|'})
		if !ok {
			return fail()
		}
		return r.MaskRepeat(next, tokenizer.SpaceByteMask, 0)
	case PipeTableSeparator ^ tokenizer.Open:
		s, ok := r.MaskRepeat(s, tokenizer.SpaceByteMask, 0)
		tokenizer.Assertf(ok, "MaskRepeat must match because minCount is zero")

		if next, ok := r.Token1(s, [...]byte{'|'}); ok {
			if r.IsEmptyOrWhiteSpace(next) {
				return next, true
			}
			return s, true
		}
		return fail()
	default:
		tokenizer.Assertf(false, "unexpected djot inline token type: %v(%d)", tokenType, tokenType)
		return fail()
	}
}
